
### **Linear Models**
1. **Linear Regression**  
   - Originated in statistics; first rigorous formulation by **Francis Galton** (1886).  
   - Modern usage formalized in "The Advanced Theory of Statistics" by Kendall and Stuart.

2. **Logistic Regression**  
   - **"On the Correlation of Binary Variables Measured by the Multiple Logistic Function"**  
     - By Joseph Berkson (1944).

---

### **Decision Trees**
1. **CART (Classification and Regression Trees)**  
   - **"Classification and Regression Trees"**  
     - By Breiman, Friedman, Olshen, and Stone (1984).

2. **ID3 Algorithm**  
   - **"Learning Decision Trees"**  
     - By J.R. Quinlan (1986).

3. **C4.5 Algorithm**  
   - **"C4.5: Programs for Machine Learning"**  
     - By J.R. Quinlan (1993).

---

### **Support Vector Machines**
- **"Support-Vector Networks"**  
  - By Vladimir N. Vapnik and Corinna Cortes (1995).

---

### **Clustering**
1. **K-Means Clustering**  
   - **"Some Methods for Classification and Analysis of Multivariate Observations"**  
     - By J.B. MacQueen (1967).

2. **Hierarchical Clustering**  
   - Introduced in **"Mathematical Taxonomy"** by Gower and Ross (1969).  
   - Algorithmic improvements in **"Hierarchical Clustering Schemes"** by Sneath and Sokal (1973).

3. **DBSCAN (Density-Based Spatial Clustering)**  
   - **"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise"**  
     - By Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu (1996).

---

### **Neural Networks**
1. **Perceptron**  
   - **"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain"**  
     - By Frank Rosenblatt (1958).

2. **Backpropagation**  
   - **"Learning Representations by Back-Propagating Errors"**  
     - By Rumelhart, Hinton, and Williams (1986).

3. **Convolutional Neural Networks (CNNs)**  
   - **"Gradient-Based Learning Applied to Document Recognition"**  
     - By Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner (1998).

4. **Recurrent Neural Networks (RNNs)**  
   - **"Learning Representations of Sequences with Explicit Neural Networks"**  
     - By Jeffrey Elman (1990).

---

### **Ensemble Methods**
1. **Random Forests**  
   - **"Random Forests"**  
     - By Leo Breiman (2001).

2. **AdaBoost**  
   - **"A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting"**  
     - By Yoav Freund and Robert E. Schapire (1997).

3. **Gradient Boosting**  
   - **"Greedy Function Approximation: A Gradient Boosting Machine"**  
     - By Jerome H. Friedman (2001).

---

### **Dimensionality Reduction**
1. **PCA (Principal Component Analysis)**  
   - **"On Lines and Planes of Closest Fit to Systems of Points in Space"**  
     - By Karl Pearson (1901).

2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**  
   - **"Visualizing Data Using t-SNE"**  
     - By Laurens van der Maaten and Geoffrey Hinton (2008).

---

### **Reinforcement Learning**
1. **Q-Learning**  
   - **"Learning from Delayed Rewards"**  
     - By Chris Watkins (1989).

2. **Policy Gradient Methods**  
   - **"Reinforcement Learning with Function Approximation"**  
     - By Richard S. Sutton et al. (1999).

3. **Deep Q-Learning**  
   - **"Playing Atari with Deep Reinforcement Learning"**  
     - By Volodymyr Mnih et al. (2013).

---

### **Bayesian Methods**
1. **Naive Bayes**  
   - **"A Probabilistic Theory of Pattern Recognition"**  
     - By Richard O. Duda and Peter E. Hart (1973).

2. **Bayesian Networks**  
   - **"Probabilistic Reasoning in Intelligent Systems"**  
     - By Judea Pearl (1988).

---

### **Other Significant Algorithms**
1. **K-Nearest Neighbors (KNN)**  
   - **"The Condensed Nearest Neighbor Rule"**  
     - By Peter Hart (1968).

2. **Apriori Algorithm (for Association Rules)**  
   - **"Fast Algorithms for Mining Association Rules"**  
     - By Rakesh Agrawal and Ramakrishnan Srikant (1994).

3. **Transformers**  
   - **"Attention Is All You Need"**  
     - By Vaswani et al. (2017).


---
https://jerryfriedman.su.domains/ftp/


---

# Language Models:



### **Before 2003 (Dependencies and Foundational Work)**

1. **Rosenblatt (1958) - The Perceptron**
   - *Title*: "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain"
   - Introduced the perceptron algorithm, the foundation of neural networks.

2. **Hopfield (1982) - Hopfield Networks**
   - *Title*: "Neural networks and physical systems with emergent collective computational abilities"
   - Early work on recurrent neural networks for memory-based computation.

3. **Rumelhart, Hinton, Williams (1986) - Backpropagation**
   - *Title*: "Learning Representations by Back-Propagating Errors"
   - Provided the algorithmic framework for training multilayer perceptrons (MLPs) effectively.

4. **LeCun et al. (1989) - Early CNNs**
   - *Title*: "Backpropagation Applied to Handwritten Zip Code Recognition"
   - Demonstrated the effectiveness of convolutional neural networks for image processing.

5. **Schuster and Paliwal (1997) - Bidirectional RNN**
   - *Title*: "Bidirectional Recurrent Neural Networks"
   - Introduced the concept of bidirectional RNNs, critical for sequence processing tasks.

6. **Hochreiter and Schmidhuber (1997) - LSTM**
   - *Title*: "Long Short-Term Memory"
   - First introduced the LSTM architecture, addressing the vanishing gradient problem in RNNs.

7. **Bengio et al. (2000) - Neural Language Models**
   - *Title*: "A Neural Probabilistic Language Model"
   - Pioneered the use of neural networks for probabilistic language modeling, predating Mikolov's RNN-based models.

---

### **Articles (2003–2017)**

1. **MLP, following Bengio et al. (2003)**
   - *Title*: "Learning Deep Architectures for AI"
   - Helped establish deep learning principles, particularly for multilayer perceptrons.

2. **RNN, following Mikolov et al. (2010)**
   - *Title*: "Recurrent Neural Network Based Language Model"
   - Demonstrated the effectiveness of RNNs for sequential language modeling tasks.

3. **LSTM, following Graves et al. (2014)**
   - *Title*: "Generating Sequences with Recurrent Neural Networks"
   - Applied LSTM architectures for sequence generation in music and text.

4. **GRU, following Kyunghyun Cho et al. (2014)**
   - *Title*: "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches"
   - Introduced GRU, a simpler alternative to LSTM with comparable performance.

5. **CNN, following DeepMind WaveNet (2016)**
   - *Title*: "WaveNet: A Generative Model for Raw Audio"
   - Presented a CNN-based generative model capable of producing high-fidelity audio.

6. **Transformer, following Vaswani et al. (2017)**
   - *Title*: "Attention Is All You Need"
   - Revolutionized natural language processing by introducing the transformer architecture, replacing recurrent models.

---

### **After 2017 (New Ideas and Architectures)**

1. **BERT (2018) - Devlin et al.**
   - *Title*: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   - A significant extension of the Transformer model, emphasizing bidirectional context.

2. **GPT (2018–2020) - Radford et al.**
   - *Title*: "Improving Language Understanding by Generative Pre-training"
   - GPT-2 and GPT-3 further advanced generative transformer models.

3. **XLNet (2019) - Yang et al.**
   - *Title*: "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
   - Combined the strengths of autoregressive and autoencoding transformers.

4. **T5 (2019) - Raffel et al.**
   - *Title*: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
   - Proposed a text-to-text framework for a wide range of NLP tasks.

5. **Vision Transformers (ViT) (2021) - Dosovitskiy et al.**
   - *Title*: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
   - Applied transformers to vision tasks, challenging the dominance of CNNs.

6. **Diffusion Models (2021–2022)**
   - *Title*: "Denoising Diffusion Probabilistic Models" (Ho et al., 2020)
   - Introduced diffusion models for generative tasks, rivaling GANs and VAEs.

7. **ChatGPT and Instruction-Tuned Models (2022–2023)**
   - *Title*: "Training Language Models to Follow Instructions with Human Feedback"
   - Refined conversational AI with instruction-tuned fine-tuning.



### **Recent Developments (2021–2025)**

#### **Language Models and NLP**
1. **ChatGPT (2022) - OpenAI**
   - *Title*: "Training Language Models to Follow Instructions with Human Feedback"
   - Introduced instruction-tuned GPT models, refining conversational abilities and context understanding.

2. **PaLM (2022) - Chowdhery et al.**
   - *Title*: "PaLM: Scaling Language Modeling with Pathways"
   - A massively scaled transformer model leveraging sparse activations, pushing performance on diverse NLP tasks.

3. **LLaMA (2023) - Meta AI**
   - *Title*: "LLaMA: Open and Efficient Foundation Language Models"
   - Optimized for efficiency and training, LLaMA was a step towards democratizing large-scale NLP research.

4. **Claude (2023) - Anthropic**
   - Focused on building safer, interpretable large language models with enhanced fine-tuning for human-like reasoning.

5. **GPT-4 (2023) - OpenAI**
   - Significant improvements in multimodal understanding, enabling text and image input for complex reasoning tasks.

---

#### **Vision Models**
1. **DINO (2021) - Caron et al.**
   - *Title*: "Emerging Properties in Self-Supervised Vision Transformers"
   - Introduced self-supervised learning in Vision Transformers (ViT) without labeled data.

2. **Segment Anything (2023) - Meta AI**
   - *Title*: "Segment Anything"
   - General-purpose segmentation model trained on billions of images, capable of identifying any object in an image.

3. **ControlNet (2023) - Zhang et al.**
   - *Title*: "Adding Conditional Control to Text-to-Image Diffusion Models"
   - Enhanced diffusion models for fine-grained image generation with precise control mechanisms.

---

#### **Audio and Speech**
1. **Whisper (2022) - OpenAI**
   - *Title*: "Robust Speech Recognition via Large-Scale Pretraining"
   - A multilingual speech recognition model trained on diverse datasets for improved robustness.

2. **AudioLM (2022) - Borsos et al.**
   - *Title*: "AudioLM: A Language Modeling Approach to Audio Generation"
   - Introduced transformer-based models for generating realistic audio, including speech and music.

---

#### **Multimodal Models**
1. **CLIP (2021) - Radford et al.**
   - *Title*: "Learning Transferable Visual Models From Natural Language Supervision"
   - Unified vision and language understanding by training on text-image pairs.

2. **DALL-E 2 (2022) - OpenAI**
   - *Title*: "Hierarchical Text-Conditional Image Generation with CLIP Latents"
   - Pioneered realistic, creative image generation from textual descriptions.

3. **Flamingo (2022) - DeepMind**
   - *Title*: "A Visual Language Model for Few-Shot Learning"
   - Enabled seamless image-text integration for tasks requiring cross-modal understanding.

4. **Gemini (2023) - Google DeepMind**
   - Introduced multimodal capabilities, integrating advanced text and vision reasoning.

---

#### **Generative Models**
1. **Stable Diffusion (2022) - Stability AI**
   - *Title*: "High-Resolution Image Synthesis with Latent Diffusion Models"
   - Open-sourced high-quality image generation, revolutionizing creative applications.

2. **DeepFloyd IF (2023) - Stability AI**
   - Text-to-image model achieving exceptional detail in image generation.

---

#### **Efficient Transformers**
1. **Longformer (2020) - Beltagy et al.**
   - Focused on handling long sequences efficiently for transformers.

2. **FlashAttention (2022) - Dao et al.**
   - *Title*: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
   - Accelerated training of transformer models by optimizing memory usage in attention mechanisms.

3. **Mistral (2023) - Mistral AI**
   - Lightweight, efficient transformers optimized for low-resource environments.

---

#### **Reinforcement Learning**
1. **Dreamer (2021) - Hafner et al.**
   - *Title*: "Dream to Control: Learning Behaviors by Latent Imagination"
   - Enhanced model-based reinforcement learning with latent-space imagination.

2. **AlphaCode (2022) - DeepMind**
   - *Title*: "Programming with Language Models"
   - Applied transformers for generating competitive coding solutions.
