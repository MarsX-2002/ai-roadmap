{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization may be defined as any modification or change in the learning algorithm that helps reduce its error over a test dataset, commonly known as generalization error but not on the supplied or training dataset.\n",
    "\n",
    "In learning algorithms, there are many variants of regularization techniques, each of which tries to cater to different challenges. These can be listed down straightforwardly based on the kind of challenge the technique is trying to deal with:\n",
    "\n",
    "1. Some try to put extra constraints on the learning of an ML model, like adding restrictions on the range/type of parameter values.\n",
    "2. Some add more terms in the objective or cost function, like a soft constraint on the parameter values. More often than not, a careful selection of the right constraints and penalties in the cost function contributes to a massive boost in the model's performance, specifically on the test dataset.\n",
    "3. These extra terms can also be encoded based on some prior information that closely relates to the dataset or the problem statement.\n",
    "4. One of the most commonly used regularization techniques is creating ensemble models, which take into account the collective decision of multiple models, each trained with different samples of data.\n",
    "\n",
    "The main aim of regularization is to reduce the over-complexity of the machine learning models and help the model learn a simpler function to promote generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularisation\n",
    "\n",
    "L1 regularization, also known as **Lasso Regularization** (Least Absolute Shrinkage and Selection Operator), is a technique used in machine learning to prevent overfitting and improve model generalization by adding a penalty term to the loss function. The penalty is proportional to the **absolute values** of the model's weights.\n",
    "\n",
    "#### Objective Function with L1 Regularization\n",
    "The modified loss function with L1 regularization is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{Loss}(\\hat{y}, y) + \\lambda \\sum_{i=1}^{n} |w_i|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\text{Loss}(\\hat{y}, y) $: The original loss function (e.g., mean squared error or cross-entropy).\n",
    "- $ \\lambda $: The regularization strength (hyperparameter).\n",
    "- $ w_i $: Model parameters (weights).\n",
    "\n",
    "#### Key Properties of L1 Regularization\n",
    "1. **Feature Selection**:\n",
    "   - L1 regularization drives some weights $ w_i $ to exactly **zero**.\n",
    "   - This leads to sparse models, where only the most important features are retained, effectively performing feature selection.\n",
    "\n",
    "2. **Overfitting Prevention**:\n",
    "   - By penalizing large weights, the model is less likely to overfit the training data.\n",
    "\n",
    "3. **Sparsity**:\n",
    "   - The L1 norm encourages sparsity (many weights being zero), which makes the model interpretable and efficient.\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - The value of $ \\lambda $ controls the trade-off between fitting the data and applying the regularization penalty. A larger $ \\lambda $ increases regularization strength, leading to a sparser model.\n",
    "\n",
    "#### Applications\n",
    "- Feature selection in high-dimensional datasets.\n",
    "- Regression problems (e.g., Lasso regression).\n",
    "- Models requiring interpretability, where sparsity is valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization, also known as **Ridge Regularization**, is a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function. The penalty is proportional to the **squared magnitude** of the model's weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Objective Function with L2 Regularization**\n",
    "The modified loss function with L2 regularization is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{Loss}(\\hat{y}, y) + \\lambda \\sum_{i=1}^{n} w_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\text{Loss}(\\hat{y}, y) $: The original loss function (e.g., mean squared error or cross-entropy).\n",
    "- $ \\lambda $: The regularization strength (hyperparameter).\n",
    "- $ w_i $: Model parameters (weights).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Properties of L2 Regularization**\n",
    "1. **Weight Shrinkage**:\n",
    "   - L2 regularization penalizes large weights by shrinking them toward zero, though they never become exactly zero.\n",
    "\n",
    "2. **Overfitting Prevention**:\n",
    "   - By discouraging large coefficients, the model generalizes better to unseen data.\n",
    "\n",
    "3. **Smoothness**:\n",
    "   - Unlike L1, which creates sparse models, L2 regularization ensures all features are included but with reduced impact.\n",
    "\n",
    "4. **Mathematical Stability**:\n",
    "   - L2 regularization is particularly useful when features are highly correlated or the dataset is ill-conditioned.\n",
    "\n",
    "5. **Hyperparameter Tuning**:\n",
    "   - The value of $ \\lambda $ controls the trade-off between fitting the data and regularization. A larger $ \\lambda $ increases regularization strength, making the model simpler but potentially underfitting the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Applications**\n",
    "- Stabilizing regression problems with multicollinearity (e.g., Ridge Regression).\n",
    "- Improving generalization for complex models with numerous parameters.\n",
    "- Controlling overfitting in neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand why we need any sort of regularization, let’s go through a quick example.\n",
    "\n",
    "Let’s say we wanted to predict people’s height with a dataset that included several predictors such as: weight, salary, ethnicity and eyesight. Our linear regression equation would like the one below.\n",
    "\n",
    "$$\n",
    "\\mathcal{height} = \\beta_0 + \\beta_1 \\cdot \\mathcal{weight} + \\beta_2 \\cdot \\mathcal{salary} + \\beta_3 \\cdot \\mathcal{ethnicity} + \\beta_4 \\cdot \\mathcal{eyesight}\n",
    "$$\n",
    "\n",
    "While WE KNOW some of these predictors are incorrect and would not provide any useful insight into someone’s height, linear regression would force a way in doing so by minimising the loss function, in this case, let’s say RSS (Residual Sum of Squares). This leads to overfitting, which essentially means that even the noise within the dataset is being modelled.\n",
    "\n",
    "Regularization combats this by adding a penalty term that can help disregard or weaken the coefficients of any irrelevant predictor like eyesight and salary from the model.\n",
    "\n",
    "<div style=\"display:flex;justify-content:center;\">\n",
    "<img src=\"images/overfitting.png\" width=800 style=\"background:white;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
