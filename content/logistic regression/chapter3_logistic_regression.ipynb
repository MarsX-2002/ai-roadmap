{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression** is a supervised learning algorithm that can be used to classify data into categories, or classes, by predicting the probability that an observation falls into a particular class based on its features.\n",
    "\n",
    "Though it can be extended to more than two categories, logistic regression is often used for binary classification, i.e. determining which of two groups a data point belongs to, or whether an event will occur or not.\n",
    "\n",
    "The typical setup for logistic regression is as follows: there is an outcome $y$ that falls into one of two categories (say 0 or 1), and the following equation is used to estimate the probability that $y$ belongs to a particular category given inputs $X = (x_1,x_2,\\dots,x_k)$\n",
    "\n",
    "$$\n",
    "P(y=1 \\mid X) = \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "z = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_k x_k\n",
    "$$\n",
    "\n",
    "The equation for $z$ likely looks familiar. This is called a linear predictor, and it is transformed by the sigmoid function so that the values fall between 0 and 1, and can therefore be interpreted as probabilities. This resulting probability is then compared to a threshold to predict a class for $y$ based on $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How It Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make this a bit more concrete by walking through an example. Suppose that you want to go for a hike in Seattle. You want to predict whether it will be sunny or rainy, so that you can decide whether to hike or drink coffee indoors at a local cafe. You know that it rains often in Seattle, but you’ve heard the summers have nice weather. The question is: can we predict the weather, given factors such as the temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume there are two classes: Rainy Days and Sunny Days. We can assign a numeric value of 0 and 1 to each class, say 0 to a Rainy Day and 1 to a Sunny Day. We have one continuous feature: the temperature, in degrees Fahrenheit. For each day, we can plot this value along with the corresponding temperature.\n",
    "\n",
    "<img src=\"images/weather_predict_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we should not fit a linear regression model to these data. The outcomes of a linear regression model can take any numerical value, but these data can only take on outcomes of 0 or 1, so the predictions of a linear model may not be meaningful.\n",
    "\n",
    "Instead, we can fit a logistic function to the data. The values of this function can be interpreted as probabilities, as the values range between 0 and 1. We can interpret the line as the probability of a sunny day given a particular temperature.\n",
    "\n",
    "<img src=\"images/weather_predict_2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logistic function to predict the probabilities of each outcome, we can predict the class. We use a classification threshold, or decision boundary, to decide the predicted class based on the probability of each class given the feature values. A typical threshold is 0.5, where we predict an outcome will occur if the probability of that outcome is greater than 0.5. This threshold can be adjusted — for example, if you really dislike the rain, you may want to set the threshold higher to be more cautious, so that that you predict a sunny day and go hiking only if the probability of a sunny day exceeds that threshold.\n",
    "\n",
    "<img src=\"images/weather_predict_3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting our model, the goal is to find the parameters that optimize a function that defines how well the model is performing. Put simply, the goal is to make predictions as close to 1 when the outcome is 1 and as close to 0 when the outcome is 0. In machine learning, the function to be optimized is called the loss function or cost function. We use the loss function to determine how well our model fits the data.\n",
    "\n",
    "\n",
    "A suitable loss function in logistic regression is called the Log-Loss, or binary cross-entropy. This function is:\n",
    "\n",
    "$$\n",
    "\\text{Log-Loss} = \\sum_{i=0}^{n} - \\left( y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right)\n",
    "$$\n",
    "\n",
    "where $n$ is the number of samples, indexed by $i$, $y_i$ is the true class for the index $i$, and $p_i$ is the model prediction for the index $i$. Minimizing the $\\text{Log-Loss}$ is equivalent to maximizing the Log-Likelihood, since the $\\text{Log-Loss}$ is the negative of the Log-Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the coefficients $ (\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_k) $ that minimize the loss function? There are two main approaches for logistic regression: gradient descent and maximum likelihood estimation. We’ll briefly discuss both here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to estimate coefficients is to use gradient descent. In gradient descent, the goal is to minimize the Log-Loss cost function over all samples. This method involves selecting initial parameter values, and then updating them incrementally by moving them in the direction that decreases the loss. At each iteration, the parameter value is updated by the gradient, scaled by the step size (otherwise known as the learning rate). The gradient is the vector encompassing the direction and rate of the fastest increase of a function, which can be calculated using partial derivatives. The parameters are updated in the opposite direction of the gradient by the step size in an attempt to find the parameter values that minimize the Log-Loss.\n",
    "\n",
    "Because the gradient calculates where the function is increasing, going in the opposite direction leads us to the minimum of our function. In this manner, we can repeatedly update our model's coefficients such that we eventually reach the minimum of our error function and obtain a sigmoid curve that fits our data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is finding the model that maximizes the likelihood of observing the data by using Maximum Likelihood Estimation (MLE). It turns out, minimizing the Log-Loss is equivalent to maximizing the Log-Likelihood. Therefore, the goal is to find the parameter values that maximize the following:\n",
    "\n",
    "$$\n",
    "\\text{Log-Likelihood} = \\sum_{i=0}^{n} \\left( y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right)\n",
    "$$\n",
    "\n",
    "We can do so by differentiating the Log-Likelihood with respect to the parameters, setting the derivatives equal to 0, and solving the equation to find the estimates of the parameters."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
