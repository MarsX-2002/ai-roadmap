{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;color:#0F4C81;\">\n",
    "Deep Dive into Activation Functions\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Activation functions play a crucial role in deep learning models by introducing non-linearity, enabling networks to learn complex patterns. In this tutorial, we will explore various activation functions, their properties, and when to use them.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Do We Need Activation Functions?\n",
    "Neural networks consist of multiple layers, where each neuron computes a weighted sum of its inputs and passes it through an activation function. Without activation functions, a neural network would behave like a simple linear model, regardless of the number of layers. Activation functions introduce non-linearity, allowing networks to learn complex features.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Types of Activation Functions\n",
    "\n",
    "### 2.1 Step Function\n",
    "- **Definition:** Outputs either 0 or 1 based on a threshold.\n",
    "- **Formula:** $$ f(x) = \\begin{cases} 1, & x \\geq 0 \\\\ 0, & x < 0 \\end{cases} $$\n",
    "- **Usage:** Used in early perceptron models but not suitable for deep learning due to its lack of gradient information.\n",
    "\n",
    "### 2.2 Sigmoid (Logistic) Function\n",
    "- **Formula:** $$ f(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "- **Pros:** Smooth, differentiable, and maps output to (0,1).\n",
    "- **Cons:** Suffering from vanishing gradients, leading to slow learning in deep networks.\n",
    "- **Usage:** Commonly used in the output layer of binary classification problems.\n",
    "\n",
    "### 2.3 Tanh (Hyperbolic Tangent) Function\n",
    "- **Formula:** $$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "- **Pros:** Zero-centered output in the range (-1,1), improving optimization over sigmoid.\n",
    "- **Cons:** Also suffers from vanishing gradients for large inputs.\n",
    "- **Usage:** Used in earlier deep networks before ReLU gained popularity.\n",
    "\n",
    "### 2.4 ReLU (Rectified Linear Unit)\n",
    "- **Formula:** $$ f(x) = \\max(0, x) $$\n",
    "- **Pros:** Simple, computationally efficient, and helps mitigate the vanishing gradient problem.\n",
    "- **Cons:** Can suffer from the \"dying ReLU\" problem, where some neurons become inactive (output always 0).\n",
    "- **Usage:** Most widely used activation function in hidden layers of deep networks.\n",
    "\n",
    "### 2.5 Leaky ReLU\n",
    "- **Formula:** $$ f(x) = \\begin{cases} x, & x \\geq 0 \\\\ \\alpha x, & x < 0 \\end{cases} $$\n",
    "- **Pros:** Prevents dying ReLU problem by allowing small negative outputs.\n",
    "- **Usage:** A preferred alternative to standard ReLU.\n",
    "\n",
    "### 2.6 Parametric ReLU (PReLU)\n",
    "- **Formula:** $$ f(x) = \\begin{cases} x, & x \\geq 0 \\\\ \\alpha x, & x < 0 \\end{cases} $$\n",
    "- **Pros:** Similar to Leaky ReLU but with learnable parameter \\(\\alpha\\).\n",
    "- **Usage:** Often used in computer vision models.\n",
    "\n",
    "### 2.7 ELU (Exponential Linear Unit)\n",
    "- **Formula:** $$ f(x) = \\begin{cases} x, & x \\geq 0 \\\\ \\alpha (e^x - 1), & x < 0 \\end{cases} $$\n",
    "- **Pros:** Reduces bias shift and speeds up learning.\n",
    "- **Usage:** A more advanced alternative to ReLU.\n",
    "\n",
    "### 2.8 Swish\n",
    "- **Formula:** $$ f(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}} $$\n",
    "- **Pros:** Smooth and often outperforms ReLU.\n",
    "- **Usage:** Used in Googleâ€™s EfficientNet models.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Choosing the Right Activation Function\n",
    "| Activation Function | Pros | Cons | Best Used In |\n",
    "|----------------------|------|------|--------------|\n",
    "| Sigmoid | Probabilistic interpretation | Vanishing gradients | Output layer of binary classifiers |\n",
    "| Tanh | Zero-centered | Vanishing gradients | Some RNNs |\n",
    "| ReLU | Simple, efficient | Dying neurons | Most deep networks |\n",
    "| Leaky ReLU | Prevents dying neurons | Adds a small computational cost | Alternative to ReLU |\n",
    "| PReLU | Learnable slope | More parameters to train | Deep networks with high complexity |\n",
    "| ELU | Reduces bias shift | Slightly slower than ReLU | Advanced deep learning tasks |\n",
    "| Swish | Smoother than ReLU | More computationally expensive | EfficientNet models |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusion\n",
    "Activation functions are essential for deep learning, enabling networks to learn complex patterns. While ReLU remains the most popular choice, alternatives like Swish and ELU are gaining traction. Choosing the right activation function depends on the problem, architecture, and computational constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "1. McCulloch, W. S., & Pitts, W. (1943). *A Logical Calculus of the Ideas Immanent in Nervous Activity*.\n",
    "2. Rosenblatt, F. (1958). *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*.\n",
    "3. Nair, V., & Hinton, G. E. (2010). *Rectified Linear Units Improve Restricted Boltzmann Machines*.\n",
    "4. He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*.\n",
    "5. Ramachandran, P., Zoph, B., & Le, Q. V. (2017). *Searching for Activation Functions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
